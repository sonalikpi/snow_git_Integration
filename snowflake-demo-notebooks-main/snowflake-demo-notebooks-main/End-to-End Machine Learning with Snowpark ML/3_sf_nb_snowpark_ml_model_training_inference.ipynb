{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": [
        "# Select Packages\n",
        "\n",
        "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
        "\n",
        "- snowflake-ml-python\n",
        "- seaborn\n",
        "- matplotlib\n",
        "\n",
        "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
      ],
      "id": "5d00dfa5-aaa7-4ae0-bff6-b77df47bcdb6"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell2"
      },
      "source": [
        "## 3. ML Modeling\n",
        "\n",
        "- In this notebook, we will illustrate how to train an XGBoost model with the diamonds dataset using the [Snowpark ML Modeling API](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling). \n",
        "- We also show how to do inference and manage models via Model Registry or as a UDF (See Appendix).\n",
        "\n",
        "The Snowpark ML Model API supports `scikit-learn`, `xgboost`, and `lightgbm` models."
      ],
      "id": "88ad6fb5-6d82-47c0-b8f0-5470d4c393be"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell3"
      },
      "source": [
        "### Import Libraries"
      ],
      "id": "efba96f4-156f-414d-9400-f72bcd8ddbd5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": "# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.functions import udf\nimport snowflake.snowpark.functions as F\n\n# Snowpark ML\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n\n# misc\nimport json\nimport joblib\nimport cachetools\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')",
      "id": "91b6df09-63fe-4038-b029-dfb5abd776c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell5"
      },
      "outputs": [],
      "source": [
        "-- Using Warehouse, Database, and Schema created in tutorial #1\n",
        "USE WAREHOUSE ML_HOL_WH;\n",
        "USE DATABASE ML_HOL_DB;\n",
        "USE SCHEMA ML_HOL_SCHEMA;"
      ],
      "id": "bbf79c3e-97e2-41d3-82c7-7e52a68c999f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "# Establish Secure Connection to Snowflake\n",
        "session = get_active_session()"
      ],
      "id": "98602c72-c86c-4266-8515-6e0c3a6d9b8a"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell7"
      },
      "source": [
        "### Load the data & preprocessing pipeline"
      ],
      "id": "eaab22f2-c362-4ec5-85e1-103082e80c77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell8"
      },
      "outputs": [],
      "source": [
        "# Load in the data\n",
        "diamonds_df = session.table(\"DIAMONDS\")\n",
        "diamonds_df"
      ],
      "id": "7ff39875-9784-43d8-8135-35db94f64665"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "# Categorize all the features for modeling\n",
        "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
        "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
        "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "LABEL_COLUMNS = ['PRICE']\n",
        "OUTPUT_COLUMNS = ['PREDICTED_PRICE']"
      ],
      "id": "ca447267-d754-4f37-a888-ed3f950d112c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell10"
      },
      "outputs": [],
      "source": [
        "# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n",
        "# where our notebook is running, and then load it using joblib.\n",
        "session.file.get('@ML_HOL_ASSETS/preprocessing_pipeline.joblib.gz', '/tmp')\n",
        "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\n",
        "preprocessing_pipeline = joblib.load(PIPELINE_FILE)"
      ],
      "id": "ab3dcfe3-64a4-431a-8aed-49d77c849252"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell11"
      },
      "source": [
        "### Build a simple XGBoost Regression model\n",
        "\n",
        "What's happening here? \n",
        "\n",
        "- The `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a [Snowpark Optimized Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized) if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n",
        "- The `model.predict()` function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.\n",
        "\n",
        "You can check the query history once you execute the following cell to check."
      ],
      "id": "7384a5fa-086f-466c-900e-104e1302199e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "diamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n",
        "\n",
        "# Run the train and test sets through the Pipeline object we defined earlier\n",
        "train_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\n",
        "test_df = preprocessing_pipeline.transform(diamonds_test_df)"
      ],
      "id": "39853aa6-696d-4966-8cd9-ab4b10c4a0ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "# Define the XGBRegressor\n",
        "regressor = XGBRegressor(\n",
        "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
        "    label_cols=LABEL_COLUMNS,\n",
        "    output_cols=OUTPUT_COLUMNS\n",
        ")\n",
        "\n",
        "# Train\n",
        "regressor.fit(train_df)\n",
        "\n",
        "# Predict\n",
        "result = regressor.predict(test_df)"
      ],
      "id": "40828bb7-0541-4e21-ab0c-ca1c558c31d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell14"
      },
      "outputs": [],
      "source": [
        "# Just to illustrate, we can also pass in a Pandas DataFrame to Snowpark ML's model.predict()\n",
        "regressor.predict(test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].to_pandas())"
      ],
      "id": "71b096d6-0003-4409-860f-03ac960d7715"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell15"
      },
      "source": [
        "Let's analyze the results using Snowpark ML's MAPE."
      ],
      "id": "b4e53ef4-d99e-4c43-a1d5-4b14ec17cc5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell16"
      },
      "outputs": [],
      "source": [
        "mape = mean_absolute_percentage_error(df=result, \n",
        "                                        y_true_col_names=\"PRICE\", \n",
        "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
        "\n",
        "result.select(\"PRICE\", \"PREDICTED_PRICE\")"
      ],
      "id": "0710b88b-1698-4638-94a9-a3b1337c2b66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell17"
      },
      "outputs": [],
      "source": [
        "print(f\"Mean absolute percentage error: {mape}\")"
      ],
      "id": "5974d66e-088d-45ec-8a68-a1c44d56d024"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "# Plot actual vs predicted \n",
        "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
        "g.ax.axline((0,0), slope=1, color=\"r\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "572f437e-fbc0-4cb2-9066-e5755e3fe39a"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell19"
      },
      "source": [
        "### Now, let's use Snowpark ML's **Distributed** `GridSearchCV()` function to find optimal model parameters\n",
        "\n",
        "We will increase the warehouse size to scale up our hyperparameter tuning to take advantage of parallelized model training to accelerate this search."
      ],
      "id": "81db1e71-9f94-4ea0-82d9-4c49392cbb88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=LARGE;"
      ],
      "id": "8c4523df-051b-47be-a3f4-6c17fd5923bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(\n",
        "    estimator=XGBRegressor(),\n",
        "    param_grid={\n",
        "        \"n_estimators\":[100, 200, 300, 400, 500],\n",
        "        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    },\n",
        "    n_jobs = -1,\n",
        "    scoring=\"neg_mean_absolute_percentage_error\",\n",
        "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
        "    label_cols=LABEL_COLUMNS,\n",
        "    output_cols=OUTPUT_COLUMNS\n",
        ")\n",
        "\n",
        "# Train\n",
        "grid_search.fit(train_df)"
      ],
      "id": "d528112f-0be9-434a-83aa-2947e277a7db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell22"
      },
      "outputs": [],
      "source": [
        "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=XSMALL;"
      ],
      "id": "8d8615da-dcb1-4def-91dc-a1afb97be9b6"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell23"
      },
      "source": [
        "We see that the best estimator has the following parameters: `n_estimators=500` & `learning_rate=0.4`.\n",
        "\n",
        "We can use `to_sklearn()` in order to get the actual xgboost model object, which gives us access to all its attributes."
      ],
      "id": "5eb1139f-08a8-4c9f-a210-dd617be50cd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell24"
      },
      "outputs": [],
      "source": [
        "print(grid_search.to_sklearn().best_estimator_)"
      ],
      "id": "09854ae0-24a4-4133-bf92-095d1abbeffb"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell25"
      },
      "source": [
        "We can also analyze the grid search results."
      ],
      "id": "de3dc8a0-aca1-496d-a76b-3dd165d88f19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell26"
      },
      "outputs": [],
      "source": [
        "# Analyze grid search results\n",
        "gs_results = grid_search.to_sklearn().cv_results_\n",
        "n_estimators_val = []\n",
        "learning_rate_val = []\n",
        "for param_dict in gs_results[\"params\"]:\n",
        "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
        "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
        "mape_val = gs_results[\"mean_test_score\"]*-1\n",
        "\n",
        "gs_results_df = pd.DataFrame(data={\n",
        "    \"n_estimators\":n_estimators_val,\n",
        "    \"learning_rate\":learning_rate_val,\n",
        "    \"mape\":mape_val})\n",
        "\n",
        "sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "0600af1f-424d-49b9-be96-c01db0694444"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell27"
      },
      "source": [
        "This is consistent with the `learning_rate=0.4` and `n_estimator=500` chosen as the best estimator with the lowest MAPE."
      ],
      "id": "7e108b85-bf4f-4c0a-be83-66ab5d194415"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell28"
      },
      "source": [
        "Now, let's predict and analyze the results from using the best estimator."
      ],
      "id": "422ff4ab-92ce-42e4-b3e9-92d8bc551ddc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell29"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "result = grid_search.predict(test_df)\n",
        "\n",
        "# Analyze results\n",
        "mape = mean_absolute_percentage_error(df=result, \n",
        "                                        y_true_col_names=\"PRICE\", \n",
        "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
        "\n",
        "result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n",
        "print(f\"Mean absolute percentage error: {mape}\")"
      ],
      "id": "8db69e45-b0a7-4d96-9417-781d1f6b3cc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell30"
      },
      "outputs": [],
      "source": [
        "# Plot actual vs predicted \n",
        "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
        "g.ax.axline((0,0), slope=1, color=\"r\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "07030287-f020-4f0e-8c5e-077781022633"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell31"
      },
      "source": [
        "Let's save our optimal model and its metadata:\n"
      ],
      "id": "de1df12f-9783-48df-ae4c-6ed875a95a14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell32"
      },
      "outputs": [],
      "source": [
        "optimal_model = grid_search.to_sklearn().best_estimator_\n",
        "optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\n",
        "optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n",
        "\n",
        "optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n",
        "                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]"
      ],
      "id": "09b8f388-6a74-4907-ba8b-a1625eb0dc86"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell33"
      },
      "source": [
        "### Manage models using Model Registry"
      ],
      "id": "9ea2435e-bcfe-46ff-a630-872be3cb3622"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell34"
      },
      "source": [
        "Now, with Snowpark ML's [Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry), we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately execute batch inference tasks in a Snowflake warehouse or deploy to a Snowpark Container Service."
      ],
      "id": "905b456b-95c0-4e18-9eea-296c291eafaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell35"
      },
      "source": [
        "First, we will log our models.\n",
        "\n",
        "Refer to [this Medium post](https://medium.com/snowflake/whats-in-a-name-model-naming-versioning-in-snowpark-model-registry-b5f7105fd6f6) on best practices for model naming & versioning conventions."
      ],
      "id": "4ab80ada-ff3d-402a-8abc-6b4849e552dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell36"
      },
      "outputs": [],
      "source": [
        "# Get sample input data to pass into the registry logging function\n",
        "X = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n",
        "\n",
        "db = identifier._get_unescaped_name(session.get_current_database())\n",
        "schema = identifier._get_unescaped_name(session.get_current_schema())\n",
        "\n",
        "# Define model name\n",
        "model_name = \"DIAMONDS_PRICE_PREDICTION\"\n",
        "\n",
        "# Create a registry and log the model\n",
        "native_registry = Registry(session=session, database_name=db, schema_name=schema)\n",
        "\n",
        "# Let's first log the very first model we trained\n",
        "model_ver = native_registry.log_model(\n",
        "    model_name=model_name,\n",
        "    version_name='V0',\n",
        "    model=regressor,\n",
        "    sample_input_data=X, # to provide the feature schema\n",
        ")\n",
        "\n",
        "# Add evaluation metric\n",
        "model_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n",
        "\n",
        "# Add a description\n",
        "model_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n",
        "\n",
        "# Now, let's log the optimal model from GridSearchCV\n",
        "model_ver2 = native_registry.log_model(\n",
        "    model_name=model_name,\n",
        "    version_name='V1',\n",
        "    model=optimal_model,\n",
        "    sample_input_data=X, # to provide the feature schema\n",
        ")\n",
        "\n",
        "# Add evaluation metric\n",
        "model_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n",
        "\n",
        "# Add a description\n",
        "model_ver2.comment = \"This is the second iteration of our Diamonds Price Prediction model \\\n",
        "                        where we performed hyperparameter optimization.\""
      ],
      "id": "4a525afd-650f-443d-89ee-e2ff7024646c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell37"
      },
      "outputs": [],
      "source": [
        "# Let's confirm they were added\n",
        "native_registry.get_model(model_name).show_versions()"
      ],
      "id": "6c24f6d8-dc45-4e0b-bf17-8f1eb25104d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell38"
      },
      "source": [
        "We can see what the default model is when we have multiple versions with the same model name:\n"
      ],
      "id": "9e139033-da10-4f13-959a-98538dcac547"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell39"
      },
      "outputs": [],
      "source": [
        "native_registry.get_model(model_name).default.version_name"
      ],
      "id": "a209db8b-4e12-4866-9d5d-b8c063aca5cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell40"
      },
      "source": [
        "Now we can use the optimal model to perform inference."
      ],
      "id": "17a774c1-9ed2-4676-bcef-1a4bc99b27bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell41"
      },
      "outputs": [],
      "source": [
        "model_ver = native_registry.get_model(model_name).version('v1')\n",
        "result_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n",
        "result_sdf2.show()"
      ],
      "id": "cf04908d-9cb9-4084-99c7-2b7905535587"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell42"
      },
      "source": [
        "You can also execute inference using SQL. To do this, we will use a SQL cell and reference our model's predict method via the model object's name, e.g. `DIAMONDS_PRICE_PREDICTION!predict`."
      ],
      "id": "23b82429-87d5-41a6-beb6-f2787b37ebd8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell43"
      },
      "outputs": [],
      "source": [
        "test_df.write.mode('overwrite').save_as_table('diamonds_test', table_type=\"temporary\")"
      ],
      "id": "e3c4c864-c5aa-4234-b2e9-8a7b8243638f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell44"
      },
      "outputs": [],
      "source": [
        "--- for the default version:\n",
        "SELECT a.*, DIAMONDS_PRICE_PREDICTION!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['PREDICTED_PRICE'] as prediction from diamonds_test a"
      ],
      "id": "1312c300-b288-4f60-9554-305810c7e041"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell45"
      },
      "outputs": [],
      "source": [
        "--- for any other version (for example V1 below):\n",
        "WITH model_version_alias AS MODEL DIAMONDS_PRICE_PREDICTION VERSION v1 SELECT a.*, model_version_alias!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['output_feature_0'] as prediction from diamonds_test a"
      ],
      "id": "646fb2f2-5358-4954-aa9e-dbaf4d4700e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell46"
      },
      "source": [
        "Let's do some clean up now."
      ],
      "id": "ad1ef059-c310-4dcf-9305-99fe17173820"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell47"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "native_registry.delete_model(model_name)"
      ],
      "id": "7dbcedb9-712c-4da7-8a52-f1b645fa1c1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell48"
      },
      "outputs": [],
      "source": [
        "# Confirm it was deleted\n",
        "native_registry.show_models()"
      ],
      "id": "96b67c88-8b26-4a81-a9c1-8d9f08506e13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell49"
      },
      "source": [
        "## Conclusion\n",
        "Congratulations, you have successfully completed this quickstart! Through this quickstart, we were able to showcase Snowpark for Machine Learning through the introduction of Snowpark ML, the Python library and underlying infrastructure for data science and machine learning tasks. Now, you can run data preprocessing, feature engineering, model training, and batch inference in a few lines of code without having to define and deploy stored procedures that package scikit-learn, xgboost, or lightgbm code.\n",
        "\n",
        "For more information, check out the resources below:\n",
        "\n",
        "### Related Resources\n",
        "- [Snowpark ML API Docs](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)\n",
        "- [Getting Started with Data Engineering and ML Using Snowpark](https://quickstarts.snowflake.com/guide/getting_started_with_dataengineering_ml_using_snowpark_python/index.html?index=..%2F..index#0)\n",
        "- [Advanced: Snowpark for Python Data Engineering Guide](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_python/index.html)\n",
        "- [Advanced: Snowpark for Python Machine Learning Guide](https://quickstarts.snowflake.com/guide/getting_started_snowpark_machine_learning/index.html)\n",
        "- [Snowpark for Python Demos](https://github.com/Snowflake-Labs/snowpark-python-demos/blob/main/README.md)\n",
        "- [Snowpark for Python Developer Docs](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n"
      ],
      "id": "161c5609-8ec5-41e0-90df-5f43fb87e063"
    }
  ]
}